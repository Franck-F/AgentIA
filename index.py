import os
import re
import streamlit as st
import chromadb
from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts.chat import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers.multi_query import MultiQueryRetriever
from rank_bm25 import BM25Okapi
from langchain.schema import BaseRetriever
from typing import List
from langchain.schema import Document
from pydantic import Field, PrivateAttr

# Configuration API OpenAI
OPENAI_API_KEY = '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'
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# Initialisation de ChromaDB
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("rag_chatbot")

# chargement et indexation des donn√©es 
def load_and_index_pdfs(directory):
    loader = DirectoryLoader(directory, loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        chunk_size=1000,
        embedding_ctx_length=8191
    )
    db = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    return db

# Chargement des documents
pdf_directory = "data/pdfs"  
if os.path.exists(pdf_directory):
    vector_db = load_and_index_pdfs(pdf_directory)
else:
    vector_db = Chroma(persist_directory="./chroma_db", embedding_function=OpenAIEmbeddings())

# D√©finition du prompt
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""R√¥le & Mission
Tu es un assistant virtuel sp√©cialis√© dans l‚Äôorientation acad√©mique et professionnelle des √©l√®ves et √©tudiants en France.
Ton objectif est de les guider dans :
‚Ä¢ Le choix de leurs √©tudes et de leur carri√®re en fonction de leurs int√©r√™ts et comp√©tences.
‚Ä¢ L‚Äôanalyse des offres de formation et d‚Äôemploi pour identifier celles qui leur correspondent le mieux.
‚Ä¢ L‚Äôaccompagnement dans la r√©daction de documents essentiels : 
  o Lettres de motivation.
  o Projets de formation motiv√©s (notamment pour Parcoursup).
  o CV et autres documents de candidature.
‚Ä¢ Le suivi des candidatures, en aidant √† organiser un journal de candidatures et √† optimiser leurs d√©marches.
Tu dois toujours fournir des informations pr√©cises, adapt√©es et √† jour, exclusivement pour la France.

M√©thodologie de r√©ponse
Lorsqu‚Äôun utilisateur pose une question, tu dois :
1. Comprendre pr√©cis√©ment la demande et poser des questions de clarification.
2. Fournir une r√©ponse claire, d√©taill√©e et adapt√©e au niveau et au contexte de l‚Äôutilisateur.
3. S‚Äôadapter au ton de l‚Äôutilisateur.
4. Structurer tes r√©ponses pour une meilleure lisibilit√©.
5. Ne pas inventer d‚Äôinformations.

Exemples de demandes
‚Ä¢ "Peux-tu m'expliquer comment fonctionne Parcoursup ?"
‚Ä¢ "Comment structurer une lettre de motivation efficace ?"
‚Ä¢ "Quelle est la r√©mun√©ration d‚Äôun apprenti en France ?"
‚Ä¢ "propose moi des formations apr√®s le baccalaur√©at "


Limites et restrictions
1. Pas de conseils m√©dicaux ou juridiques.
2. Pas d‚Äô√©v√©nements r√©cents au-del√† de la date de mise √† jour.
3. Pas d‚Äôopinions personnelles ou d‚Äô√©motions.

Question: {question}""",
)

# Cr√©ation d'un retriever personnalis√© avec BM25
class BM25EnhancedRetriever(BaseRetriever):
    _vectorstore: object = PrivateAttr()
    _k: int = PrivateAttr()
    _base_retriever: object = PrivateAttr()
    
    def __init__(self, vectorstore, k: int = 4):
        super().__init__()
        self._vectorstore = vectorstore
        self._k = k
        self._base_retriever = vectorstore.as_retriever(search_kwargs={"k": k * 2})
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        # R√©cup√©rer les documents via le retriever de base
        docs = self._base_retriever.get_relevant_documents(query)
        
        if not docs:
            return []
            
        # Pr√©parer les documents pour BM25
        corpus = [doc.page_content for doc in docs]
        tokenized_corpus = [doc.split() for doc in corpus]
        
        # Cr√©er et entra√Æner le mod√®le BM25
        bm25 = BM25Okapi(tokenized_corpus)
        
        # Tokeniser la requ√™te
        tokenized_query = query.split()
        
        # Calculer les scores BM25
        bm25_scores = bm25.get_scores(tokenized_query)
        
        # Trier les documents selon les scores BM25
        scored_docs = list(zip(docs, bm25_scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # Retourner les k meilleurs documents
        return [doc for doc, score in scored_docs[:self._k]]
    
    async def aget_relevant_documents(self, query: str) -> List[Document]:
        return self.get_relevant_documents(query)

# retriever avec BM25
retriever = MultiQueryRetriever.from_llm(
    BM25EnhancedRetriever(vector_db),
    ChatOpenAI(),
    prompt=QUERY_PROMPT
)

# RAG prompt template
template = """R√©pond √† la question en utilisant uniquement le contexte suivant:
{context}
Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

# Cr√©ation de la cha√Æne RAG
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | ChatOpenAI()
    | StrOutputParser()

)

# Fonction pour d√©tecter si une question de clarification est n√©cessaire
def needs_clarification(query):
    llm = ChatOpenAI(temperature=0.7)
    clarification_prompt = ChatPromptTemplate.from_template("""
    Analyse la question suivante et d√©termine si elle n√©cessite des clarifications.
    Question: {query}
     
    R√©ponds par "True" si la question est vague ou n√©cessite plus de contexte,
    "False" si la question est claire et sp√©cifique.
    R√©ponse (True/False uniquement):
    """)
     
    response = llm(clarification_prompt.format_messages(query=query))
    return response.content.strip().lower() == "true"

# Fonction pour g√©n√©rer des questions de clarification
def generate_clarifying_questions(query):
    llm = ChatOpenAI(temperature=0.7)
    question_prompt = ChatPromptTemplate.from_template("""
    En tant qu'assistant d'orientation, g√©n√®re 2-3 questions pertinentes pour mieux comprendre le contexte de la demande suivante:
    {query}
    
    Format: Retourne uniquement les questions, une par ligne.
    """)
    
    response = llm(question_prompt.format_messages(query=query))
    return response.content.strip().split('\n')

# Fonction pour nettoyer la r√©ponse
def clean_response(response):
    if not response or response.strip() == "":
        return "Je ne trouve pas de r√©ponse pertinente √† votre question. Pourriez-vous la reformuler ou me donner plus de d√©tails ?"
    return response.strip()

# Fonction pour d√©tecter les formules de politesse
def detect_greeting(text):
    greetings = {
        "bonjour": "Bonjour",
        "bonsoir": "Bonsoir",
        "salut": "Salut",
        "hey": "Hey",
        "coucou": "Coucou",
        "hello": "Hello"
    }
    
    first_words = text.lower().split()[:2]
    for word in first_words:
        if word in greetings:
            return greetings[word]
    return None

# Fonction pour g√©n√©rer une r√©ponse compl√®te
def generate_complete_response(query, conversation_history=None):
    llm = ChatOpenAI(temperature=0.7)
    
    # Construire le contexte √† partir de l'historique des conversations
    context = ""
    if conversation_history:
        # Prendre les 5 derniers √©changes pour garder le contexte pertinent
        recent_history = conversation_history[-5:]
        context = "\n".join([
            f"{'User' if i%2==0 else 'Assistant'}: {msg}" 
            for i, msg in enumerate(recent_history)
        ])
    
    # Cr√©er un prompt qui inclut l'historique et guide la r√©ponse
    response_prompt = ChatPromptTemplate.from_template("""
    Tu es un assistant sp√©cialis√© dans l'orientation scolaire et professionnelle.
    
    Historique de la conversation:
    {context}
    
    Question actuelle: {query}
    
    En tenant compte de l'historique de la conversation et de la question actuelle:
    1. Analyse le contexte global de la discussion
    2. Assure-toi que ta r√©ponse est coh√©rente avec les informations pr√©c√©demment donn√©es
    3. Fais r√©f√©rence aux √©l√©ments pertinents des √©changes pr√©c√©dents si n√©cessaire
    4. Si la question actuelle contredit ou remet en question des informations pr√©c√©dentes, 
       explique poliment la contradiction et clarifie la situation
    
    R√©ponds de mani√®re claire, pr√©cise et personnalis√©e:
    """)
    
    # G√©n√©rer la r√©ponse en utilisant le contexte
    messages = response_prompt.format_messages(
        context=context if context else "Pas d'historique disponible",
        query=query
    )
    response = llm(messages)
    
    # Nettoyer et retourner la r√©ponse
    return {
        "response": clean_response(response.content),
        "needs_clarification": False
    }

# Fonction principale de conversation
def conversation_chat(query):
    try:
        # V√©rifier les salutations
        greeting = detect_greeting(query)
        if greeting and len(query.split()) <= 2:
            return {
                "response": f"{greeting}, comment puis-je vous aider ?",
                "needs_clarification": False
            }

        # G√©rer les pr√©cisions
        if st.session_state.awaiting_clarification and not needs_clarification(query):
            st.session_state.clarification_context.append(query)
            
            # G√©n√©rer la r√©ponse finale avec le contexte
            final_response = generate_complete_response(
                st.session_state.original_query,
                st.session_state.conversation_history
            )
            
            # R√©initialiser l'√©tat
            st.session_state.awaiting_clarification = False
            st.session_state.original_query = None
            st.session_state.clarification_context = []
            
            return {
                "response": final_response["response"],
                "needs_clarification": False
            }

        # Nouvelle question n√©cessitant des pr√©cisions
        if needs_clarification(query):
            st.session_state.awaiting_clarification = True
            st.session_state.original_query = query
            clarifying_questions = generate_clarifying_questions(query)
            return {
                "response": "Pour mieux vous aider, j'aurais besoin de quelques pr√©cisions :\n" + "\n".join(clarifying_questions),
                "needs_clarification": True
            }

        # Question directe
        response_data = generate_complete_response(query, st.session_state.conversation_history)
        if greeting:
            response_data["response"] = f"{greeting}! {response_data['response']}"
        return response_data

    except Exception as e:
        return {
            "response": f"Je suis d√©sol√©, mais je rencontre une difficult√© : {str(e)}",
            "needs_clarification": False
        }

# Initialisation des variables de session
if "messages" not in st.session_state:
    st.session_state.messages = []
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []
if "awaiting_clarification" not in st.session_state:
    st.session_state.awaiting_clarification = False
if "original_query" not in st.session_state:
    st.session_state.original_query = None
if "clarification_context" not in st.session_state:
    st.session_state.clarification_context = []

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Bacc2Futur",
    page_icon=":book:",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Styles CSS personnalis√©s
st.markdown("""
<style>
.chat-container {
    display: flex;
    margin-bottom: 1rem;
    align-items: flex-start;
    max-width: 800px;
}

.user-message {
    justify-content: flex-start;
    margin-right: 20%;
}

.assistant-message {
    flex-direction: row-reverse;
    margin-left: 20%;
}

.avatar {
    width: 40px;
    height: 40px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    margin: 0 10px;
    font-size: 20px;
}

.user-avatar {
    background-color: #E8F5E9;
    color: #2E7D32;
    border: 2px solid #2E7D32;
}

.assistant-avatar {
    background-color: #E3F2FD;
    color: #1565C0;
    border: 2px solid #1565C0;
}

.message-content {
    padding: 1rem;
    border-radius: 15px;
    max-width: 80%;
}

.user-content {
    background-color: #E8F5E9;
    color: #1a1a1a;
    border: 1px solid #C8E6C9;
}

.assistant-content {
    background-color: #E3F2FD;
    color: #1a1a1a;
    border: 1px solid #BBDEFB;
}

.stTextInput>div>div>input {
    border-radius: 20px;
}
</style>
""", unsafe_allow_html=True)

# Interface utilisateur
col1, col2, col3 = st.columns([1,2,1])
with col2:
    st.markdown("<h1 style='text-align: center;'>üìñ Bacc2Futur - Assistant d'orientation acad√©mique</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center;'>Bienvenue sur Bacc2Futur, un assistant pour vous aider √† trouver la formation et l'orientation qui vous convient.</p>", unsafe_allow_html=True)

# Affichage des messages
for message in st.session_state.messages:
    if message["role"] == "user":
        st.markdown(f"""
            <div class="chat-container user-message">
                <div class="avatar user-avatar">üë§</div>
                <div class="message-content user-content">
                    {message["content"]}
                </div>
            </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
            <div class="chat-container assistant-message">
                <div class="avatar assistant-avatar">ü§ñ</div>
                <div class="message-content assistant-content">
                    {message["content"]}
                </div>
            </div>
        """, unsafe_allow_html=True)

# Zone de saisie
if prompt := st.chat_input("Posez votre question ici"):
    # Message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.markdown(f"""
        <div class="chat-container user-message">
            <div class="avatar user-avatar">üë§</div>
            <div class="message-content user-content">
                {prompt}
            </div>
        </div>
    """, unsafe_allow_html=True)

    # R√©ponse de l'assistant
    response_data = conversation_chat(prompt)
    st.session_state.messages.append({"role": "assistant", "content": response_data["response"]})
    st.markdown(f"""
        <div class="chat-container assistant-message">
            <div class="avatar assistant-avatar">ü§ñ</div>
            <div class="message-content assistant-content">
                {response_data["response"]}
            </div>
        </div>
    """, unsafe_allow_html=True)
    st.session_state.conversation_history.extend([prompt, response_data["response"]])
